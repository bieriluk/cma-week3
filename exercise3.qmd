---
title: "expercise3"
author: "Lukas Bieri (bieriluk)"
date: "07.05.2023"
format: html
editor: source
---
# Exercise 3 -  Computational Movement Analysis: Patterns and Trends in Environmental Data

## 1. Information & Metadata

### 1.1 Information

**Study program:** MSc in Environment and Natural Resources FS23

**Module:** Computational Movement Analysis: Patterns and Trends in Environmental Data

**Course:** R Exercise 3 - Week 3

**Lecturers:** Patrick Laube (laup), Nils Ratnaweera (rata) & Dominic Lüönd (luoe)

**Authors:** Lukas Bieri (bieriluk)


### 1.1 Structure R-Project

The following project was created for the exercise:
cma-week3-rexercise

Data was stored in the folder "datasets".

Code and explanations in this Quarto-File where stored and submitted via the public GitHub Repo "bieriluk/cma-week3":
https://github.com/bieriluk/cma-week3.git


### 1.2 Data

Data for the exercise was provided by the teaching staff via the plattform "Moodle". 

The data originates from the ZHAW Research Projekt "Using an acoustic signal to prevent wild boars from damaging crops"
https://www.zhaw.ch/en/about-us/news/news-releases/news-detail/event-news/using-an-acoustic-signal-to-prevent-wild-boars-from-damaging-crops/

The following datasets were used:

1.  File: "wildschwein_BE_2056.csv"
    Downloaded: 28.04.2023
    Source: https://moodle.zhaw.ch/pluginfile.php/1168373/mod_folder/content/0/wildschwein_BE_2056.csv?forcedownload=1
    
In addition, tracking data of my own movement was used. This data was collected using the tracking app Posmo: https://posmo.coop/produkte/posmo-project-tracking-fuer-gruppen

The tracking data was downloaded on the 02.05.2023 and covers the duration from 11.04.-07.05.2023.
2.  File: "posmo_2023-01-01_2023-05-07.csv"

Information on the R Exercise can be found here:
https://computationalmovementanalysis.github.io/FS23/

These R Exercises are created by Patrick Laube, Nils Ratnaweera, Nikolaos Bakogiannis and Dominic Lüönd for the Course Computational Movement Analysis and are licensed under Creative Commons Attribution 4.0 International License.


### 1.4 Used Software

**R version 4.2.1 (2022-06-23 ucrt)** -- "Funny-Looking Kid"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

**RStudio 2023.03.0+386** "Cherry Blossom" Release (3c53477afb13ab959aeb5b34df1f10c237b256c3, 2023-03-09) for Windows
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) RStudio/2023.03.0+386 Chrome/108.0.5359.179 Electron/22.0.3 Safari/537.36

**Git for Windows Version 2.40.1**


## 2. Performe the Segmentation on the exemplary wild boar data (Demo)

### 2.1 Preparation

In Preparation: Restart R and clear console.

Then: Load necessary functions

```{r}
library("readr") # to import tabular data (e.g. csv)
library ("tidyr")
library("dplyr") # to manipulate (tabular) data
library("ggplot2") # to visualize data
library("sf")
#install.packages("SimilarityMeasures")
library("SimilarityMeasures")
```


### 2.2 Import & filter wild boar data
```{r}
wildschwein_BE <- read_delim("datasets/wildschwein_BE_2056.csv", ",")

wildschwein_BE$TierName |> unique()
```

For this, we will only work with the data of "sabi"
```{r}
sabi <- wildschwein_BE |> 
  filter(TierName == "Sabi") |> 
  filter(DatetimeUTC >= as.POSIXct("2015-07-01", tz = "UTC"), 
         DatetimeUTC < as.POSIXct("2015-07-03", tz = "UTC"))

sabi
```

### 2.3 Get an overview
```{r}
ggplot(sabi, aes(E,N, color = DatetimeUTC)) +
  geom_point() +
  geom_path() +
  coord_equal()
```

```{r}
sabi |> 
  head(50) |> 
  ggplot(aes(DatetimeUTC, 1)) +
  geom_point()
```

### 2.4 (a) Specify a temporal windows v for in which to measure Euclidean distances
Set at 2 previous and two following data point, so overall appr. 1h window.

### 2.5 (b) Measure the distance from every point to every other point within this temporal window v
```{r}
sabi <- sabi |> 
  mutate(
    n_plus1 = sqrt((lead(E,1) - E)^2 + (lead(N,1) - N)^2),
    n_plus2 = sqrt((lead(E,2) - E)^2 + (lead(N,2) - N)^2),
    n_minus1 = sqrt((lag(E,1) - E)^2 + (lag(N,1) - N)^2),
    n_minus2 = sqrt((lag(E,2) - E)^2 + (lag(N,2) - N)^2)
  )
```

```{r}
sabi <- sabi |> 
  rowwise() |> 
  mutate(
    stepMean = mean(c(n_minus1, n_minus2, n_plus1, n_plus2))
  ) |> 
  ungroup()
```
Careful, you need rowwise() for the mutate() function to caclulate mean() for every row. You need to ungroup it in the end, because your data frame has the rowwise grouping saved.

### 2.6 (c) Remove “static points”: These are points where the average distance is less than a given threshold

We need to determine the appropriate threshold where the points are stationary. It helps to visualize the data:
```{r}
ggplot(sabi, aes(stepMean)) +
  geom_histogram(binwidth = 10) +
  geom_vline(xintercept = mean(sabi$stepMean, na.rm = TRUE))
```

In this example we used the mean as the threshold (below the mean = static = TRUE).
```{r}
sabi <- sabi |>
    ungroup() |>
    mutate(static = stepMean < mean(stepMean, na.rm = TRUE))

sabi
```

```{r}
ggplot(sabi, aes(E,N)) +
  geom_path() +
  geom_point(aes(color = static)) +
  coord_equal()
```

If the result looks good, filter out the static segments:
```{r}
sabi_filter <- sabi |>
    filter(!static)

sabi_filter
```


### 2.7 (d) Remove short subtrajectories
Not implemented for this example.

### 2.8 Visualize the segmented & filtered data
And visualize the result:
```{r}
sabi_filter |>
    ggplot(aes(E, N)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "bottom")
```


## 3. Segmentation of your Posmo data
With what we learned, we can no process the Posmo data

### 3.1 Preperation
Import the data:
```{r}
posmo <- read_delim("datasets/posmo_2023-01-01T00 00 00+01 00-2023-05-07T23 59 59+02 00.csv", ",")
```

Keep only the necessary columns
```{r}
posmo <- select(posmo, datetime, lon_x, lat_y)
```

Store your data frame as a spatial data frame and transform the coordinate system from WGS84 (i.e. EPSG 4326) to CH1903+ LV95 (EPSG 2056)
```{r}
posmo <- st_as_sf(posmo, coords = c("lon_x","lat_y"), crs = 4326) |>
  st_transform(2056)

head(posmo)
```

Extract the coordinates into separate coloms to use them for euclidean distance calculation:
```{r}
posmo_coordinates <- st_coordinates(posmo)

posmo <- cbind(posmo, posmo_coordinates)
```

Choose a single day for this exercise (13.04.2023, because many different visited places that day) and filter your data:
```{r}
posmo_filter <- posmo |>
    filter(as.Date(datetime) == "2023-04-13")
```

### 3.2 Task 1: Segmentation
Implement above mentioned steps a-c for your movement data.

#### 3.2.1 Get an overview
```{r}
ggplot(posmo_filter, aes(X,Y, color = datetime)) +
  geom_point() +
  geom_path() +
  coord_equal()
```
X = E, Y = N

```{r}
posmo_filter|> 
  head(50) |> 
  ggplot(aes(datetime, 1)) +
  geom_point()
```

#### 3.2.2 (a) Specify a temporal windows v for in which to measure Euclidean distances
Calculate timelag to figure out a appropriate temporal window:
```{r}
posmo_filter <- posmo_filter|>
  mutate(timelag_s = as.numeric(difftime(lead(datetime), datetime)))
```

So what does the timelag between measurement points look like:
```{r}
tail(posmo_filter)
mean(posmo_filter$timelag_s, na.rm = TRUE)
median(posmo_filter$timelag_s, na.rm = TRUE)
min(posmo_filter$timelag_s, na.rm = TRUE)
max(posmo_filter$timelag_s, na.rm = TRUE)

posmo_filter|> 
  ggplot(aes(timelag_s)) +
  geom_histogram(binwidth = 1) +
  lims(x = c(0, 20000)) +
  scale_y_log10() +
  scale_x_log10()

posmo_filter |> 
  ggplot(aes(datetime, timelag_s)) +
  geom_point() + 
  geom_line()

```
Unfortunately the timelag between measurement points is very irregular. Maybe in a later step it would make sense to increase the granularity in the data to 

For this exercise we try to use the data as is and try it with a window of 10 steps and 20 steps forward and backward in the data. This is roughly v = 20s-6min, because the most common timelag is 10s, with most being between 1 and 20s. 

#### 3.2.3 (b) Measure the distance from every point to every other point within this temporal window v
```{r}
posmo_filter <- posmo_filter |> 
  mutate(
    n_plus10 = sqrt((lead(X,10) - X)^2 + (lead(Y,10) - Y)^2),
    n_plus20 = sqrt((lead(X,20) - X)^2 + (lead(Y,20) - Y)^2),
    n_minus10 = sqrt((lag(X,10) - X)^2 + (lag(Y,10) - Y)^2),
    n_minus20 = sqrt((lag(X,20) - X)^2 + (lag(Y,20) - Y)^2)
  )
```

```{r}
posmo_filter <- posmo_filter |> 
  rowwise() |> 
  mutate(
    stepMean = mean(c(n_minus10, n_minus20, n_plus10, n_plus20))
  ) |> 
  ungroup()
```
Careful, you need rowwise() for the mutate() function to caclulate mean() for every row. You need to ungroup it in the end, because your data frame has the rowwise grouping saved.

#### 3.2.4 (c) Remove “static points”: These are points where the average distance is less than a given threshold

We need to determine the appropriate threshold where the points are stationary. It helps to visualize the data:
```{r}
ggplot(posmo_filter, aes(stepMean)) +
  geom_histogram(binwidth = 10) +
  geom_vline(xintercept = median(posmo_filter$stepMean, na.rm = TRUE))
```



### 3.3 Task 2: Specify and apply threshold d
To find a good threshold, I tried different distances which I would realistically travel when not being stationary.
```{r}
posmo_filter <- posmo_filter |>
    ungroup() |>
    mutate(static = stepMean < 200) |>
    drop_na(static)
```

### 3.4 Task 3: Visualize segmented trajectories
```{r}
ggplot(posmo_filter, aes(X,Y)) +
  geom_path() +
  geom_point(aes(color = static)) +
  coord_equal()
```
Moving 200m in approx. 6min (more or less the time window) being the threshold for being stationary seems to line up well with the reality when visualizing it.

### 3.5 Task 4: Segment-based analysis
Create function for segmentation at the stationary points:
```{r}
rle_id <- function(vec) {
    x <- rle(vec)$lengths
    as.factor(rep(seq_along(x), times = x))
}
```

Run the function on the data:
```{r}
posmo_filter <- posmo_filter |>
    mutate(segment_id = rle_id(static))

head(posmo_filter)
```

Visualize the segments:
```{r}
ggplot(posmo_filter, aes(X,Y)) +
  geom_path() +
  geom_point(aes(color = segment_id)) +
  coord_equal()
```

Seems to line up well with ground truth for that day and the trips I took. If I would want to improve the segmentation, I would have to compare the details with reality and maybe smoothen out the differences in timelags.

### 3.6 Task 5: Similarity measures

```{r}
pedestrian <- read_delim("datasets/pedestrian.csv", ",")
```

```{r}
ggplot(pedestrian, aes(E,N, color = TrajID)) +
  geom_path() +
  geom_point() +
  coord_equal() +
  facet_wrap(TrajID~.)
```

### 3.7 Task 6: Calculate similarity
Separate into different matrices:
```{r}
Traj1 <- pedestrian |>
  filter(TrajID == 1) |> 
  select(E, N) |> 
  data.matrix()
class(Traj1)

Traj2 <- pedestrian |>
  filter(TrajID == 2) |> 
  select(E, N) |> 
  data.matrix()

Traj3 <- pedestrian |>
  filter(TrajID == 3) |> 
  select(E, N) |> 
  data.matrix()

Traj4 <- pedestrian |>
  filter(TrajID == 4) |> 
  select(E, N) |> 
  data.matrix()

Traj5 <- pedestrian |>
  filter(TrajID == 5) |> 
  select(E, N) |> 
  data.matrix()

Traj6 <- pedestrian |>
  filter(TrajID == 6) |> 
  select(E, N) |> 
  data.matrix()

```

Before calculating the similarities, by eye I would consider 2 and 6 to be the most similar to 1, followed by 3 the 5. 4 being the most dissimilar.

Set up the ID's of the trajectories for the matrices:
```{r}
ID <- c(2, 3, 4, 5, 6)
```


DTW
```{r}
DTW(Traj1, Traj2)
DTW(Traj1, Traj3)
DTW(Traj1, Traj4)
DTW(Traj1, Traj5)
DTW(Traj1, Traj6)

DTW <- c(DTW(Traj1, Traj2), DTW(Traj1, Traj3), DTW(Traj1, Traj4), DTW(Traj1, Traj5), DTW(Traj1, Traj6))
DTW
```

EditDist
```{r}
EditDist(Traj1, Traj2, 20)
EditDist(Traj1, Traj3, 20)
EditDist(Traj1, Traj4, 20)
EditDist(Traj1, Traj5, 20)
EditDist(Traj1, Traj6, 20)

EditDist <- c(EditDist(Traj1, Traj2, 20), EditDist(Traj1, Traj3, 20), EditDist(Traj1, Traj4, 20), EditDist(Traj1, Traj5, 20), EditDist(Traj1, Traj6, 20))
EditDist
```


Frechet 
```{r}
Frechet(Traj1, Traj2)
Frechet(Traj1, Traj3)
Frechet(Traj1, Traj4)
Frechet(Traj1, Traj5)
Frechet(Traj1, Traj6)

Frechet <- c(Frechet(Traj1, Traj2), Frechet(Traj1, Traj3), Frechet(Traj1, Traj4), Frechet(Traj1, Traj5), Frechet(Traj1, Traj6))
Frechet
```

LCSS
```{r}
LCSS(Traj1, Traj2, errorMarg = 10)
LCSS(Traj1, Traj3, errorMarg = 10)
LCSS(Traj1, Traj4, errorMarg = 10)
LCSS(Traj1, Traj5, errorMarg = 10)
LCSS(Traj1, Traj6, errorMarg = 10)

LCSS <- c(LCSS(Traj1, Traj2, errorMarg = 10), LCSS(Traj1, Traj3, errorMarg = 10), LCSS(Traj1, Traj4, errorMarg = 10), LCSS(Traj1, Traj5, errorMarg = 10), LCSS(Traj1, Traj6, errorMarg = 10))
LCSS
```

Table:
```{r}
Similarity <- data.frame(ID, DTW, EditDist, Frechet, LCSS)
Similarity
```

Change to long table & visualize:
```{r}
Similarity_long <- pivot_longer(Similarity, cols = c("DTW", "EditDist", "Frechet", "LCSS"))

ggplot(Similarity_long, aes(ID, value)) +
  geom_bar(stat='identity', aes(fill = factor(ID))) + 
  facet_wrap(name~., scales = "free_y") +
  theme(legend.position = "none") +
  labs(
    x = "Comparison trajectory",
    y = "Value", 
    title = "Computed similarities using different measures between trajectory 1 to all the other trajectories"
    )
```
According to this visualization, the closed fit to my intuitive answer would be DTW and Frechet-Distance, where 2 and 6 are the most similar in trajectory. However the both consider 3 to be the most dissimilar, which seems counter-intuitive. The only one that does not consider 3 to be very dissimilar is LCSS, but it considers it the most similar which cannot be true either. Maybe the specifications for the methods need to be adjusted

### 3.8 Submission
Submitted on the 09.05.2023 by pushing it to GitHub.


